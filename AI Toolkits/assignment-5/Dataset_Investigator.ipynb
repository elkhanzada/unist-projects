{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwbNEoO1zzEY"
      },
      "source": [
        "## Code for Downloading train and validation dataset.\n",
        "#### train_url: url of \"train.json\"\n",
        "#### valid_url: url of \"valid.json\"\n",
        "    - These are list of dictionaries.\n",
        "    - Each dictionary has key of 'user_id', 'sentence'.\n",
        "    - There are **total eight** twitter users.\n",
        "\n",
        "#### valid4test_url: url of \"valid.json\", but **without** label data. I save this as \"test.json\" for simulating grading process.\n",
        "    - This is also a list of dictionaries.\n",
        "    - But in here, each dictionary does not have a key of 'user_id', it only has 'sentence' information.\n",
        "    - For grading, I have my own 'test.json' data, which I would not provide to students. \n",
        "    - My 'test.json' has the same format as this file (only 'sentence' as a key). \n",
        "    - For simulating the grading phase, in this Colab notebook, I would save this file as a 'test.json'.\n",
        "    - In your code to be submitted, what you need to do is\n",
        "        (1) load './test.json', which is a list of dictionaries.\n",
        "        (2) predict \"user_id\" for each \"sentence\" in each dictionary.\n",
        "        (2-1) Which means you need to submit (a) code for test (b) trained model.\n",
        "        (3) add your predicted \"user_id\" information to each dictionary,\n",
        "        (4) save the final list of dictionary as a \"result.json\". "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "JAFZqyCqwZ-T",
        "outputId": "72ec117e-6a6e-476d-88fb-a01d44fd6814"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1QV7r1Gr6Qh8lB-cV5Zui5_2ElQoQgYbb\n",
            "To: /home/elkhan/AI Toolkits/assignment-5/train.json\n",
            "100%|██████████| 818k/818k [00:00<00:00, 3.40MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1MmDF2k4s7VrlWRqyOtw-KG5pHF9P7u9v\n",
            "To: /home/elkhan/AI Toolkits/assignment-5/valid.json\n",
            "100%|██████████| 127k/127k [00:00<00:00, 834kB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1T5UFbIWq8IA5ox0upGcpxtTRyJwakxwI\n",
            "To: /home/elkhan/AI Toolkits/assignment-5/test.json\n",
            "100%|██████████| 112k/112k [00:00<00:00, 931kB/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'./test.json'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gdown\n",
        "\n",
        "train_url = 'https://drive.google.com/uc?id=1QV7r1Gr6Qh8lB-cV5Zui5_2ElQoQgYbb'\n",
        "valid_url = 'https://drive.google.com/uc?id=1MmDF2k4s7VrlWRqyOtw-KG5pHF9P7u9v'\n",
        "valid4test_url = 'https://drive.google.com/uc?id=1T5UFbIWq8IA5ox0upGcpxtTRyJwakxwI'\n",
        "\n",
        "# These are the dataset you need to use for your training and validation.\n",
        "gdown.download(train_url, './train.json')\n",
        "gdown.download(valid_url, './valid.json')\n",
        "\n",
        "# Save the validation dataset in the name of 'test.json', just to simulate the grading phase. \n",
        "# You need to submit code that can \n",
        "gdown.download(valid4test_url, './test.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFNp-u-e4P-1"
      },
      "source": [
        "## What you need to submit\n",
        "1. Model `.pth` which finished training.\n",
        "2. A code (`test.py` or `test.ipynb`) which can...\n",
        "    - (a) Load `./test.json` file.\n",
        "        - Your code should work when I paste `test.json` file to the same directory where your `test.py` or `test.ipynb` is.\n",
        "    - (b) Load your pretrained model.\n",
        "    - (c) Do prediction: get `user_id` for each `sentence`\n",
        "    - (d) Save your list of dictionaries as `result.json`.\n",
        "    ```\n",
        "    # This is just an example to help your understanding.\n",
        "    d0 = {'user_id':2, 'sentence':'Hi I am Bill'}\n",
        "    d1 = {'user_id':5, 'sentence':'Hi I am Elon'}\n",
        "    ...\n",
        "    res = [d0, d1, d2, ....]\n",
        "    json.dump(res, open(result.json, 'w'))\n",
        "    ``` "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrkAaUfkyFKe",
        "outputId": "973dd3c2-0413-4f02-8480-06b3ef11676e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of test data:  800\n",
            "Example Data: \n",
            " {'sentence': 'i got arrested beaten left bloody and unconscious but i havent given up and you can not give up an inspiring read from civil rights legend'}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# load json\n",
        "input_path = './test.json'\n",
        "input_data = json.load(open(input_path, 'r'))\n",
        "print('Number of test data: ', len(input_data))\n",
        "print('Example Data: \\n', input_data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "dataset = pd.read_json(\"train.json\")\n",
        "num_classes = len(dataset['user_id'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def tokenize_pad_sequences(text, max_words, max_len):\n",
        "    '''\n",
        "    This function tokenize the input text into sequnences of intergers and then\n",
        "    pad each sequence to the same length\n",
        "    '''\n",
        "\n",
        "    # Text tokenization\n",
        "    tokenizer = Tokenizer(num_words=max_words, lower=True, split=' ')\n",
        "    tokenizer.fit_on_texts(text)\n",
        "    # Transforms text to a sequence of integers\n",
        "    X = tokenizer.texts_to_sequences(text)\n",
        "    # Pad sequences to the same length\n",
        "    X = pad_sequences(X, padding='post', maxlen=max_len)\n",
        "    # return sequences\n",
        "    return X, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_len = 50\n",
        "meta = pd.read_json(\"meta.json\")\n",
        "X, tokenizer = tokenize_pad_sequences(dataset['sentence'], max_words=len(meta['tokens']), max_len=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNNmodel(nn.Module):\n",
        "    def __init__(self, lstm_dim, num_classes, max_len):\n",
        "        super(RNNmodel, self).__init__()\n",
        "        self.lstm_dim = lstm_dim\n",
        "        self.num_classes = num_classes\n",
        "        self.max_len = max_len\n",
        "        self.char_embedding = nn.Embedding(num_embeddings=num_classes, \n",
        "                                           embedding_dim=lstm_dim)\n",
        "        self.lstm = nn.LSTM(input_size=lstm_dim, \n",
        "                            hidden_size=lstm_dim,\n",
        "                            num_layers=1, \n",
        "                            batch_first=True,\n",
        "                            )\n",
        "        \n",
        "        self.out_linear = nn.Linear(lstm_dim, num_classes)\n",
        "\n",
        "    def forward(self, sort_input, sort_output, sort_length):\n",
        "        ## originally, recommended to use torch.nn.utils.rnn.pack_padded_sequence,when we have variable lengths\n",
        "        ## but in this case, I just neglected it because beginners can be more confused with this\n",
        "        lstm_input = self.char_embedding(sort_input)\n",
        "        lstm_out, (h, c) = self.lstm(lstm_input)\n",
        "        out = self.out_linear(lstm_out)\n",
        "        \n",
        "        return nn.functional.softmax(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UL-_xAl595E"
      },
      "source": [
        "### This is just a code to simulate grading process. \n",
        "- This code just fills in the random integer index as the predicted `user_id`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab_size = 5000\n",
        "embedding_size = 32\n",
        "epochs=20\n",
        "learning_rate = 0.1\n",
        "decay_rate = learning_rate / epochs\n",
        "momentum = 0.8\n",
        "model = RNNmodel(256, num_classes, max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwkavWe9zu4B"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def random_answer(input_data, num_users=8):\n",
        "    # You need to save \"list\" of \"dictionaries\" as an \"result.json\"\n",
        "    result = list()\n",
        "    for d in input_data:\n",
        "        # Each dictionary needs to have \"sentence\" and \"user_id\"\n",
        "        tmp_result = dict()\n",
        "        tmp_result['sentence'] = d['sentence']\n",
        "\n",
        "        # This code just fill in the random integer to the answer. \n",
        "        '''\n",
        "        Do something for your prediction \n",
        "        '''\n",
        "        # Your TODO is to train a model which can fill in this 'user_id' with your own answer.\n",
        "        tmp_result['user_id'] = np.random.randint(low=0, high=num_users) # Change with your prediction\n",
        "\n",
        "        result.append(tmp_result)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2Fm7-0x292J"
      },
      "outputs": [],
      "source": [
        "random_result = random_answer(input_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mP5yfX0K3MI8"
      },
      "outputs": [],
      "source": [
        "## save with indent=2\n",
        "json.dump(random_result, open('./result.json', 'w'), indent=2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pdl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "d19aa5a20e88fe80248d4b0c2d7d27a7c9d54896ad8b5c06765854ce70c72cb9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
