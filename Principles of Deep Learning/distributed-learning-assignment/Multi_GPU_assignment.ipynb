{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4kCX2R27SPY"
      },
      "source": [
        "# Distributed Learning Assignment\n",
        "\n",
        "--------\n",
        "\n",
        "- This assignment is due on **Nov. 20 (11:59 PM)** - LATE SUBMISSION WILL NOT BE ACCEPTED\n",
        "- Please **carefully follow the instructions** for each assignment.\n",
        "\n",
        "### Grading criteria\n",
        "1. All the codes in this notebook should be runnable.\n",
        "2. Each answer variable is 0.5 point; total 3 points available. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ywo3TwFi73ds"
      },
      "source": [
        "\n",
        "# Data Parallel / Distributed Data Parallel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkdRyYuNFWmY"
      },
      "source": [
        "- Please answer the following questions:\n",
        "\n",
        "    Q1. [True or False] Data Parallel is multi-threading and Distributed Data Parallel is multi-processing that both supports multi-server training.\n",
        "\n",
        "    Q2. [True or False] Data Parallel has a memory allocation issue while Distributed Data Parallel does not. \n",
        "\n",
        "    Q3. [True or False] In Data Parallel, each GPU computes loss independently after forward pass, and all the gradients are computed during backprop that are shared with other GPUs to update models. \n",
        "\n",
        "    Q4. We would like to use DataParallel for training a model using first, thrid, and fifth GPU on the server (out of 8 GPUs). What is the correct argument for applying Data Parallel on the code below? Note that the order of GPU starts from 0.\n",
        "\n",
        "    ```\n",
        "    model = torch.nn.DataParallel(model, device_ids=Q4_ANSWER)\n",
        "    ```\n",
        "\n",
        "    Q5. Suppose we are using distributed training using 6 GPUs on a single server, where the main file name is `main.py`, and the latest PyTorch version is downloaded. We would like to use TorchElastic. What is the correct argument for distributed training below? \n",
        "\n",
        "    ```\n",
        "    $ torchrun --Q5_1_ANSWER --Q5_2ANSWER main.py\n",
        "    ```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cGxJJVthFXwi"
      },
      "outputs": [],
      "source": [
        "# Please put the correct answer for each variable.\n",
        "Q1_ANSWER = True\n",
        "\n",
        "Q2_ANSWER = False\n",
        "\n",
        "Q3_ANSWER = False\n",
        "\n",
        "Q4_ANSWER = [0,2,4]\n",
        "\n",
        "Q5_1_ANSWER = 'standalone'\n",
        "Q5_2_ANSWER = 'nproc_per_node=6'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFL-6-at_U86"
      },
      "source": [
        "# Testing: DO NOT EDIT THE CELLS BELOW AND DO NOT ADD CELLS BELOW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfN8P9fbPnJY"
      },
      "source": [
        "### DP/DDP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RISF2aiBX-zV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "False\n",
            "False\n",
            "[0, 2, 4]\n",
            "standalone\n",
            "nproc_per_node=6\n"
          ]
        }
      ],
      "source": [
        "print(Q1_ANSWER)\n",
        "print(Q2_ANSWER)\n",
        "print(Q3_ANSWER)\n",
        "print(Q4_ANSWER)\n",
        "print(Q5_1_ANSWER)\n",
        "print(Q5_2_ANSWER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NLfxyHOtPxz2"
      },
      "outputs": [],
      "source": [
        "# THIS CELL IS INTENTIONALLY LEFT AS BLANK."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3.9.10 ('pdl')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "d19aa5a20e88fe80248d4b0c2d7d27a7c9d54896ad8b5c06765854ce70c72cb9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
