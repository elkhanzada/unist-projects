{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PgZx_Sf_cur"
      },
      "source": [
        "# Cousre : IE408_AI502_IE511\n",
        "### 2022.09.16\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGq5wOIZAQTY"
      },
      "source": [
        "# Model selection via K-fold cross validation\n",
        "\n",
        "## Table of Contents\n",
        "---\n",
        "- Custom dataset <br>\n",
        "- $K$-fold cross validation (Assigment)<br>\n",
        "- Custom activation function (Assigment)<br>\n",
        "- MLP construction<br>\n",
        "- Hyperparameter tuning<br>\n",
        "\n",
        "<br>\n",
        "\n",
        "**`Assigment`** : Complete the two cell marked with (Assignment)\n",
        "- (Assignment) $K$-fold cross validation\n",
        "- (Assignment) Activation function with implementing forward and backward step\n",
        "- ðŸš¨ Please **do not modify** code that is not an Assigment cell\n",
        "- ðŸš¨ Please **do not add** any cells\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9DDDg4PdeRB"
      },
      "source": [
        "# import library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PymY1HXxJV90"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "\n",
        "from torch.autograd import Function  # to create custom activation function\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "su3fHNn4QjxS"
      },
      "source": [
        "# Boston house prices dataset specification\n",
        "\n",
        "<img src = https://user-images.githubusercontent.com/43310063/188207759-db4dad3f-31e2-4fd2-9f71-7eaaf3f88329.png>\n",
        "\n",
        "### Data examples\n",
        "<img src = https://user-images.githubusercontent.com/43310063/188208483-f6dc9c50-3399-4607-af6b-cc3a444895e8.png>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9p21KQBwdo_g"
      },
      "source": [
        "# Custom dataset in PyTorch\n",
        "### If you make *custom dataset*, you can use dataloader in pytorch\n",
        "<br>\n",
        "\n",
        "PyTorch `DATASETS` & `DATALOADERS` : <br>\n",
        "\n",
        "- `torch.utils.data.DataLoader` and `torch.utils.data.Dataset` that allow you to use pre-loaded datasets as well as your own data. <br>\n",
        "- `Dataset` stores the samples and their corresponding labels.\n",
        "- `DataLoader` wraps an iterable around the Dataset to enable easy access to the samples.\n",
        "<br><br>\n",
        "\n",
        "- Creating a Custom Dataset for your files<br>\n",
        "    - A custom Dataset class must implement three functions: `__init__`, `__len__,` and `__getitem__`.\n",
        "    - The `__init__` function is run once when instantiating the Dataset object. We initialize the directory containing the images, the annotations file, and both transforms (covered in more detail in the next section).\n",
        "    - The `__len__` function returns the number of samples in our dataset.\n",
        "    - The `__getitem__` function loads and returns a sample from the dataset at the given index `idx`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UWgFc7HlwvTq"
      },
      "outputs": [],
      "source": [
        "\n",
        "class BostonDataset(Dataset):  # from torch.utils.data import Dataset\n",
        "    def __init__(self, features, targets, train_mean=None, train_std=None):\n",
        "        self.features = torch.Tensor(features)\n",
        "        self.targets = torch.Tensor(targets).reshape(-1, 1)\n",
        "\n",
        "        # Standard Scaler using train_data's mean and train_data's std\n",
        "        if (train_mean is not None) and (train_std is not None):\n",
        "            self.features = (self.features - train_mean) / train_std\n",
        "\n",
        "    def __len__(self):  # return length of dataset\n",
        "        return len(self.targets)\n",
        "\n",
        "    def __getitem__(self, idx):  # return data with index(idx)\n",
        "        X = self.features[idx, :] \n",
        "        y = self.targets[idx]\n",
        "        \n",
        "        return X, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyyO6erTDtD6"
      },
      "source": [
        "# $K$-fold cross validation\n",
        "\n",
        "- Lecture note has details (Week2-MLP)<br>\n",
        "\n",
        "- $K$-fold cross validation procedure\n",
        "    1. Divide the training dataset into k-parts\n",
        "    2. Use k-1 parts as training set and 1 part as validation set\n",
        "    3. Repeat the procedure K times, rotating the validation set\n",
        "    4. Average validation errors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzW6JscQaZpJ"
      },
      "source": [
        "# (Assignment) $K$-fold cross validation\n",
        "\n",
        "- Write your code in loops \n",
        "- Do not use any external library\n",
        "\n",
        "<br>\n",
        "\n",
        "return `rets` that is list <br> <br>\n",
        "`rets` form : \\\\\n",
        "ã€€ã€€ã€€ã€€[[train_index_list_1, validation_index_list_1], \\\\\n",
        "ã€€ã€€ã€€ã€€[train_index_list_2, validation_index_list_2], \\\\\n",
        "ã€€ã€€ã€€ã€€... \\\\\n",
        "ã€€ã€€ã€€ã€€[train_index_list_k, validation_index_list_k]] \\\\\n",
        "<br>\n",
        "len(`rets`) : k \\\\\n",
        "`rets`[0] : [train_index_list, validation_index_list]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ZjeuxS0_JWYz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold size: 80\n",
            "80\n",
            "324\n",
            "80\n",
            "324\n",
            "80\n",
            "324\n",
            "80\n",
            "324\n",
            "80\n",
            "324\n",
            "[[80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79]]\n"
          ]
        }
      ],
      "source": [
        "def k_fold_data(dataset, k):\n",
        "    rets = [] # list will have k-fold data, example rets[[train_index_list_1, validation_index_list_1], ... [train_index_list_k, validation_index_list_k]]\n",
        "    fold_size = len(dataset) // k\n",
        "    print(f\"Fold size: {fold_size}\")\n",
        "    for i in range(k):\n",
        "        #### TODO : WRITE YOUR CODE IN THIS LOOP & COMPLETE k-fold ####\n",
        "        validation_idx = [idx for idx in range(i*fold_size,i*fold_size+fold_size)]\n",
        "        train_idx = [idx for idx in range(len(dataset)) if idx not in validation_idx]\n",
        "        \n",
        "        print(len(validation_idx))\n",
        "        print(len(train_idx))\n",
        "        \n",
        "        rets.append([train_idx, validation_idx])\n",
        "\n",
        "        # form of return variable :\n",
        "        # len(rets) : k \n",
        "        # rets[0] : [train_index_list, validation_index_list]\n",
        "        # example : \n",
        "        #   rets : [[[fold_size, ... len of dataset], [0, 1, .. fold_size-1],\n",
        "        #           [train_index_list_2, validation_index_list_2],\n",
        "        #           ...\n",
        "        #           [train_index_list_k, validation_index_list_k]]\n",
        "        #######################################################\n",
        "    return rets\n",
        "rets = k_fold_data(train_dataset,5)\n",
        "print(rets[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZw4iuUiaGWv"
      },
      "source": [
        "# Load dataset & Split dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEelKv0wRwD1",
        "outputId": "864096c6-8642-452a-f692-a865657e9728"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(404, 13) (102, 13) (404,) (102,)\n"
          ]
        }
      ],
      "source": [
        "# Load Boston house prices dataset\n",
        "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
        "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
        "\n",
        "X_data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
        "y_data = raw_df.values[1::2, 2]\n",
        "\n",
        "X_data = X_data.astype(np.float32)\n",
        "y_data = y_data.astype(np.float32)\n",
        "\n",
        "# Split the dataset into a training dataset and a test dataset\n",
        "test_size = 0.2\n",
        "train_len = int(X_data.shape[0] * (1-test_size))\n",
        "\n",
        "X_train, X_test = X_data[: train_len, :], X_data[train_len:, :]\n",
        "y_train, y_test = y_data[: train_len], y_data[train_len:]\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFXDJCke1OOh"
      },
      "source": [
        "# Custom activation function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IfbCvDU1SVa"
      },
      "source": [
        "## Activation function without trainable parameter\n",
        "- sigmoid activation function\n",
        "### $\\text{Sigmoid}(x) = \\sigma(x) = \\frac{1}{1+\\text{exp}(-x)}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lTSS2Sdr1iF3"
      },
      "outputs": [],
      "source": [
        "def sigmoid_(x):\n",
        "    return 1 / (1 + torch.exp(-x))\n",
        "\n",
        "class sigmoid(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return sigmoid_(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvNdaCfW1SPp"
      },
      "source": [
        "## Activation function with implementing forward and backward step\n",
        "- tanh activation function\n",
        "### $\\text{tanh} = \\frac{\\text{exp}(x) - \\text{exp}(-x)}{\\text{exp}(x) + \\text{exp}(-x)}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4gyujK6r1it7"
      },
      "outputs": [],
      "source": [
        "def tanh_(x):\n",
        "    return (torch.exp(x) - torch.exp(-x)) / (torch.exp(x) + torch.exp(-x))\n",
        "\n",
        "class tanh(Function):  # from torch.autograd import Function\n",
        "    \n",
        "    @staticmethod  # python decorator\n",
        "    def forward(ctx, x):\n",
        "        ctx.save_for_backward(x)\n",
        "        \n",
        "        output = tanh_(x)\n",
        "        \n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        # At the top of backward unpack saved_tensors and initialize all gradients w.r.t. inputs to None.\n",
        "\n",
        "        x, = ctx.saved_tensors\n",
        "\n",
        "        if ctx.needs_input_grad[0]:\n",
        "            grad_input = 1 - tanh_(x) ** 2 # derivative of tanh(x) : 1 - tanh^2(x)\n",
        "\n",
        "        return grad_input * grad_output\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPsOJexs1jRk"
      },
      "source": [
        "# (Assignment) Activation function with implementing forward and backward step\n",
        "\n",
        "- Write activation function, forward and backward step\n",
        "- Swish (paper : https://arxiv.org/abs/1710.05941)\n",
        "### $\\text{swish}(x) = x * \\text{Sigmoid}(\\beta x)$\n",
        "- $\\beta$ is a constant or trainable parameter\n",
        "- In this practice, $\\beta$ is fixed to 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RjSJ5bOx1nKQ"
      },
      "outputs": [],
      "source": [
        "class swish(Function):\n",
        "    \n",
        "    @staticmethod  # python decorator\n",
        "    def forward(ctx, x):\n",
        "        #### TODO : WRITE YOUR CODE ####\n",
        "        ctx.save_for_backward(x)\n",
        "\n",
        "        output = x * sigmoid_(x)\n",
        "        \n",
        "        return output\n",
        "        ################################\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        # At the top of backward unpack saved_tensors and initialize all gradients w.r.t. inputs to None.\n",
        "        #### TODO : WRITE YOUR CODE ####\n",
        "        x, = ctx.saved_tensors\n",
        "\n",
        "        if ctx.needs_input_grad[0]:\n",
        "            grad_input = x * sigmoid_(x) + sigmoid_(x) * (1 - x * sigmoid_(x))\n",
        "        \n",
        "        return  grad_input * grad_output # initialize all gradients w.r.t. inputs to None.\n",
        "        ################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuDwc0ZOdxJt"
      },
      "source": [
        "# Neural Network model class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "n-onkZDOLdCQ"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, n_feature, n_hidden, n_output, dropout_rate=0.2):\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(n_feature, n_hidden)\n",
        "        self.a1 = swish.apply  # activation function we implemented\n",
        "        \n",
        "        self.fc2 = nn.Linear(n_hidden, n_hidden)\n",
        "        self.a2 = swish.apply\n",
        "\n",
        "        self.fc3 = nn.Linear(n_hidden, n_output)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_rate) # dropout with dropout_rate\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.a1(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.a2(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuDS5iU4fby2"
      },
      "source": [
        "# Training \n",
        "- Training with $K$-fold\n",
        "- Hyperparameter optimization (HPO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5u0OCtce1s3V",
        "outputId": "0ed603ab-742f-4f5a-d17c-fed93a0ec8f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of hyperparameter candidates : 27\n",
            "[{'dropout_rate': 0.1, 'learning_rate': 0.001, 'weight_decay': 0.7}, {'dropout_rate': 0.1, 'learning_rate': 0.001, 'weight_decay': 0.8}, {'dropout_rate': 0.1, 'learning_rate': 0.001, 'weight_decay': 0.9}, {'dropout_rate': 0.1, 'learning_rate': 0.005, 'weight_decay': 0.7}, {'dropout_rate': 0.1, 'learning_rate': 0.005, 'weight_decay': 0.8}, {'dropout_rate': 0.1, 'learning_rate': 0.005, 'weight_decay': 0.9}, {'dropout_rate': 0.1, 'learning_rate': 0.01, 'weight_decay': 0.7}, {'dropout_rate': 0.1, 'learning_rate': 0.01, 'weight_decay': 0.8}, {'dropout_rate': 0.1, 'learning_rate': 0.01, 'weight_decay': 0.9}, {'dropout_rate': 0.2, 'learning_rate': 0.001, 'weight_decay': 0.7}, {'dropout_rate': 0.2, 'learning_rate': 0.001, 'weight_decay': 0.8}, {'dropout_rate': 0.2, 'learning_rate': 0.001, 'weight_decay': 0.9}, {'dropout_rate': 0.2, 'learning_rate': 0.005, 'weight_decay': 0.7}, {'dropout_rate': 0.2, 'learning_rate': 0.005, 'weight_decay': 0.8}, {'dropout_rate': 0.2, 'learning_rate': 0.005, 'weight_decay': 0.9}, {'dropout_rate': 0.2, 'learning_rate': 0.01, 'weight_decay': 0.7}, {'dropout_rate': 0.2, 'learning_rate': 0.01, 'weight_decay': 0.8}, {'dropout_rate': 0.2, 'learning_rate': 0.01, 'weight_decay': 0.9}, {'dropout_rate': 0.3, 'learning_rate': 0.001, 'weight_decay': 0.7}, {'dropout_rate': 0.3, 'learning_rate': 0.001, 'weight_decay': 0.8}, {'dropout_rate': 0.3, 'learning_rate': 0.001, 'weight_decay': 0.9}, {'dropout_rate': 0.3, 'learning_rate': 0.005, 'weight_decay': 0.7}, {'dropout_rate': 0.3, 'learning_rate': 0.005, 'weight_decay': 0.8}, {'dropout_rate': 0.3, 'learning_rate': 0.005, 'weight_decay': 0.9}, {'dropout_rate': 0.3, 'learning_rate': 0.01, 'weight_decay': 0.7}, {'dropout_rate': 0.3, 'learning_rate': 0.01, 'weight_decay': 0.8}, {'dropout_rate': 0.3, 'learning_rate': 0.01, 'weight_decay': 0.9}]\n"
          ]
        }
      ],
      "source": [
        "from itertools import product\n",
        "\n",
        "# training hyperparameters used to HPO\n",
        "hpo_vals = {  # our search space for HPO\n",
        "    'learning_rate': [0.001, 0.005, 0.01],\n",
        "    'weight_decay' : [0.7, 0.8, 0.9],\n",
        "    'dropout_rate' : [0.1, 0.2, 0.3],\n",
        "}\n",
        "hp_candidates = []\n",
        "\n",
        "items = sorted(hpo_vals.items())\n",
        "keys, vals = zip(*items)\n",
        "\n",
        "# product do cartesian product, so it creates all combinations\n",
        "for v in product(*vals):  \n",
        "    hp_candidates.append(dict(zip(keys, v)))\n",
        "\n",
        "print(\"Length of hyperparameter candidates :\", len(hp_candidates))\n",
        "print(hp_candidates)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2NpiPTvK11KX"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# model hyperparameters\n",
        "n_features = X_train.shape[1] # the number of features in input\n",
        "n_hidden = 256\n",
        "n_output = 1\n",
        "\n",
        "# training hyperparameters\n",
        "batch_size = 32\n",
        "num_epochs = 20\n",
        "k_folds = 5\n",
        "\n",
        "hpo_results = []\n",
        "\n",
        "# for standard scaler\n",
        "train_mean = X_train.mean(axis=0)  # mean for each features\n",
        "train_std = X_train.std(axis=0)  # stdandard deviation for each features\n",
        "\n",
        "train_dataset = BostonDataset(X_train, y_train, train_mean, train_std)\n",
        "test_dataset = BostonDataset(X_test, y_test, train_mean, train_std)\n",
        "\n",
        "# Loops for each hyper-parameter combination\n",
        "# Train model using k-fold with a hyper-parameter combination\n",
        "for hp in hp_candidates:\n",
        "\n",
        "    # Load hyper-parameters for training\n",
        "    learning_rate = hp[\"learning_rate\"]\n",
        "    weight_decay = hp[\"weight_decay\"]\n",
        "    dropout_rate = hp[\"dropout_rate\"]\n",
        "\n",
        "    validation_logs = [[] for _ in range(k_folds)]\n",
        "\n",
        "    # Loops for each fold\n",
        "    for fold_idx, data_idx in enumerate(k_fold_data(train_dataset, k_folds)):\n",
        "        model = MLP(n_features, n_hidden, n_output, dropout_rate).to(device) # new MLP model for each fold\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay) # use weight decay to prevent overfitting\n",
        "        criterion = nn.MSELoss() # use Mean Squared Error, because it is a regression problem that predicts house prices\n",
        "\n",
        "        train_idx, validation_idx = data_idx  # results of k_fold_data function\n",
        "\n",
        "        # divide train_dataset into train_subtset and validation_subset using the indexes that are results of k_fold_data function\n",
        "        train_subset = Subset(train_dataset, train_idx)\n",
        "        validation_subset = Subset(train_dataset, validation_idx)\n",
        "\n",
        "        train_dataloader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
        "        validation_dataloader = DataLoader(validation_subset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        validation_losses = []\n",
        "\n",
        "        model.train()\n",
        "        for epoch in range(num_epochs):\n",
        "            train_loss = 0.\n",
        "            for i, (inputs, targets) in enumerate(train_dataloader):\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "                optimizer.zero_grad() \n",
        "                # sets the gradients of all optimized torch.Tensors to zero before starting to do backpropagation\n",
        "                # By default, PyTorch accumulates the gradients. Accumulating process is convinient while training RNN\n",
        "\n",
        "                y_pred = model(inputs)\n",
        "\n",
        "                loss = criterion(y_pred, targets)\n",
        "                loss.backward()  # Computes the gradient of current tensor\n",
        "                optimizer.step()  # Performs a single optimization step (parameter update)\n",
        "\n",
        "                train_loss += loss.item() * inputs.shape[0]\n",
        "\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                validation_loss = 0.\n",
        "                for i, (inputs, targets) in enumerate(validation_dataloader):\n",
        "                    inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "                    y_pred = model(inputs)\n",
        "\n",
        "                    validation_loss += criterion(y_pred, targets).item() * inputs.shape[0]\n",
        "\n",
        "                validation_logs[fold_idx].append(validation_loss / len(validation_subset))\n",
        "\n",
        "    # Validation score is calculated by averaging the results of each folds\n",
        "    validation_score = np.mean(validation_logs)\n",
        "\n",
        "    hpo_results.append([*hp.values(), validation_score])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEOruxMVggkN"
      },
      "source": [
        "### Hyperparameter tuning results\n",
        "\n",
        "- Low validation score is better in our practice, because we use the Mean Squared Error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 896
        },
        "id": "3UIEHVOWFY_t",
        "outputId": "3c41ea17-677a-4b21-88f7-b6b7c46ecc78"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dropout_rate</th>\n",
              "      <th>learning_rate</th>\n",
              "      <th>weight_decay</th>\n",
              "      <th>validation_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.1</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.7</td>\n",
              "      <td>37.051378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.2</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.7</td>\n",
              "      <td>38.295660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.3</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.9</td>\n",
              "      <td>38.387384</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.3</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.7</td>\n",
              "      <td>39.819125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.1</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.8</td>\n",
              "      <td>40.286267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.3</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.8</td>\n",
              "      <td>41.906365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.1</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.9</td>\n",
              "      <td>42.197236</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.2</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.8</td>\n",
              "      <td>42.563753</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.2</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.9</td>\n",
              "      <td>44.084137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.1</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.7</td>\n",
              "      <td>48.398124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.3</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.7</td>\n",
              "      <td>49.359634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.1</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.8</td>\n",
              "      <td>49.979911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.2</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.7</td>\n",
              "      <td>50.416529</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.3</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.9</td>\n",
              "      <td>50.570622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.2</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.9</td>\n",
              "      <td>50.939917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.1</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.9</td>\n",
              "      <td>52.769339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.3</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.8</td>\n",
              "      <td>52.817665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.2</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.8</td>\n",
              "      <td>54.854525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.3</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.8</td>\n",
              "      <td>146.624902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.9</td>\n",
              "      <td>146.655509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.8</td>\n",
              "      <td>147.324209</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.3</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.7</td>\n",
              "      <td>147.418099</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.9</td>\n",
              "      <td>147.836212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.7</td>\n",
              "      <td>148.148104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.7</td>\n",
              "      <td>149.294749</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.3</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.9</td>\n",
              "      <td>150.929488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.8</td>\n",
              "      <td>156.671964</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    dropout_rate  learning_rate  weight_decay  validation_score\n",
              "6            0.1          0.010           0.7         37.051378\n",
              "15           0.2          0.010           0.7         38.295660\n",
              "26           0.3          0.010           0.9         38.387384\n",
              "24           0.3          0.010           0.7         39.819125\n",
              "7            0.1          0.010           0.8         40.286267\n",
              "25           0.3          0.010           0.8         41.906365\n",
              "8            0.1          0.010           0.9         42.197236\n",
              "16           0.2          0.010           0.8         42.563753\n",
              "17           0.2          0.010           0.9         44.084137\n",
              "3            0.1          0.005           0.7         48.398124\n",
              "21           0.3          0.005           0.7         49.359634\n",
              "4            0.1          0.005           0.8         49.979911\n",
              "12           0.2          0.005           0.7         50.416529\n",
              "23           0.3          0.005           0.9         50.570622\n",
              "14           0.2          0.005           0.9         50.939917\n",
              "5            0.1          0.005           0.9         52.769339\n",
              "22           0.3          0.005           0.8         52.817665\n",
              "13           0.2          0.005           0.8         54.854525\n",
              "19           0.3          0.001           0.8        146.624902\n",
              "11           0.2          0.001           0.9        146.655509\n",
              "10           0.2          0.001           0.8        147.324209\n",
              "18           0.3          0.001           0.7        147.418099\n",
              "2            0.1          0.001           0.9        147.836212\n",
              "0            0.1          0.001           0.7        148.148104\n",
              "9            0.2          0.001           0.7        149.294749\n",
              "20           0.3          0.001           0.9        150.929488\n",
              "1            0.1          0.001           0.8        156.671964"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cols = [*keys, \"validation_score\"]\n",
        "hpo_df = pd.DataFrame(hpo_results, columns=cols)\n",
        "hpo_sorted_df = hpo_df.sort_values(by=\"validation_score\")  # sort validation socre in ascending order because we used mean squared error\n",
        "hpo_sorted_df\n",
        "\n",
        "# Lower validation score is better\n",
        "# In our search space, the top row of dataframe is the best hyperparameter combination"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b--FifHX2CLh"
      },
      "source": [
        "# Train and Test model with best hyperparameter setting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GoO0rvY2Gqv"
      },
      "source": [
        "## Train model with best hyperparameter using full training dataset without $K$-fold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Em7sd0NGg7-k"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "fTwA5nl3fQ9r"
      },
      "outputs": [],
      "source": [
        "# function for plotting loss\n",
        "def plot_loss(loss_list):\n",
        "    clear_output(True) # clear output in executing cell\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    plt.ylabel(\"Train Loss\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.plot(loss_list)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "ZuTeMpkq2K-L",
        "outputId": "18cc1958-3a51-439b-abee-80b20b874f2e"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# training hyperparameters\n",
        "batch_size = 32\n",
        "num_epochs = 100\n",
        "\n",
        "# Use best hyperparameter setting, The top row of dataframe is the best hyperparameter\n",
        "learning_rate = hpo_sorted_df.iloc[0].learning_rate\n",
        "weight_decay = hpo_sorted_df.iloc[0].weight_decay\n",
        "dropout_rate = hpo_sorted_df.iloc[0].dropout_rate\n",
        "\n",
        "model_with_hpo = MLP(n_features, n_hidden, n_output, dropout_rate).to(device)\n",
        "optimizer = torch.optim.Adam(model_with_hpo.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "criterion = nn.MSELoss() # using Mean Squared Error\n",
        "\n",
        "# use full training dataset without K-fold\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "train_logs = []\n",
        "\n",
        "model_with_hpo.train()\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0.\n",
        "    for i, (inputs, targets) in enumerate(train_dataloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        y_pred = model_with_hpo(inputs)\n",
        "\n",
        "        loss = criterion(y_pred, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * inputs.shape[0]\n",
        "\n",
        "    train_logs.append(train_loss / len(train_dataset))\n",
        "    \n",
        "    if epoch % 10 == 0:\n",
        "        plot_loss(train_logs)\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3MpdrsAXtLl",
        "outputId": "62666e25-b038-4e09-ef52-85a826e62107"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(16.078529631737435, 13.513033309785447)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_logs[-1], np.array(train_logs).min()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1iSnBou2Gnf"
      },
      "source": [
        "## Test model with best hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icmjnbgF2Lfm",
        "outputId": "239d68c0-ec43-415f-a70b-bbdebcfb2286"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "average test log mse = 20.19969424079446\n"
          ]
        }
      ],
      "source": [
        "test_logs = []\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "criterion = nn.MSELoss(reduction='sum') # sum loss using Mean Squared Error\n",
        "\n",
        "model_with_hpo.eval()\n",
        "with torch.no_grad():\n",
        "    for i, (inputs, targets) in enumerate(test_dataloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        y_pred = model_with_hpo(inputs)\n",
        "        test_logs.append(criterion(y_pred, targets).item())\n",
        "\n",
        "print(f'average test log mse = {sum(test_logs)/ len(test_dataset)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GL-DlCXu6gnC"
      },
      "source": [
        "# References\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html\n",
        "- https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
        "- https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
        "- https://github.com/scikit-learn/scikit-learn/blob/36958fb24/sklearn/model_selection/_search.py#L1021\n",
        "- https://towardsdatascience.com/extending-pytorch-with-custom-activation-functions-2d8b065ef2fa\n",
        "- https://pytorch.org/docs/stable/notes/extending.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1nALDNewqHW"
      },
      "source": [
        "# Test part\n",
        "***Do not change*** this part and ***Do not add*** cell below this part<br>\n",
        "- This part is not for students\n",
        "- Do not use & add below cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOc667_fvKt2",
        "outputId": "be8fb297-8c9f-4af6-fc8b-0e7432ad9cd2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/elkhan/anaconda3/envs/pdl/lib/python3.9/site-packages/torch/autograd/gradcheck.py:652: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. \n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from torch.autograd import gradcheck\n",
        "\n",
        "input = (torch.rand(32, 13, dtype=torch.float, requires_grad=True)).to(device)\n",
        "\n",
        "test = gradcheck(model_with_hpo, input, eps=1e-3, atol=1e-2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVphOEKQwb1U",
        "outputId": "6e8345f5-d020-458f-afd8-67116029fa02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "print(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tbP_QJEw32O",
        "outputId": "09bc42ce-9771-4bf2-ae89-7548f0ca2b7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20.19969424079446\n"
          ]
        }
      ],
      "source": [
        "print(sum(test_logs)/ len(test_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBFs_cHUa-_Y",
        "outputId": "1abb33af-e85c-42e2-ca51-b827c206d1c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "k_test = 5\n",
        "k_fold_test = k_fold_data(input, k_test)\n",
        "\n",
        "check = True\n",
        "\n",
        "if len(k_fold_test) != k_test:    \n",
        "    check = False\n",
        "else:\n",
        "    for fold in k_fold_test:\n",
        "        if len(fold[1]) != (input.shape[0] // k_test):\n",
        "            check = False\n",
        "            break\n",
        "\n",
        "print(check)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.10 ('pdl')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "d19aa5a20e88fe80248d4b0c2d7d27a7c9d54896ad8b5c06765854ce70c72cb9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
