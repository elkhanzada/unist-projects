{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PgZx_Sf_cur"
      },
      "source": [
        "# Cousre : IE408_AI502_IE511\n",
        "### 2022.09.16\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGq5wOIZAQTY"
      },
      "source": [
        "# Model selection via K-fold cross validation\n",
        "\n",
        "## Table of Contents\n",
        "---\n",
        "- Custom dataset <br>\n",
        "- $K$-fold cross validation (Assigment)<br>\n",
        "- Custom activation function (Assigment)<br>\n",
        "- MLP construction<br>\n",
        "- Hyperparameter tuning<br>\n",
        "\n",
        "<br>\n",
        "\n",
        "**`Assigment`** : Complete the two cell marked with (Assignment)\n",
        "- (Assignment) $K$-fold cross validation\n",
        "- (Assignment) Activation function with implementing forward and backward step\n",
        "- ðŸš¨ Please **do not modify** code that is not an Assigment cell\n",
        "- ðŸš¨ Please **do not add** any cells\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9DDDg4PdeRB"
      },
      "source": [
        "# import library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PymY1HXxJV90"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "\n",
        "from torch.autograd import Function  # to create custom activation function\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "su3fHNn4QjxS"
      },
      "source": [
        "# Boston house prices dataset specification\n",
        "\n",
        "<img src = https://user-images.githubusercontent.com/43310063/188207759-db4dad3f-31e2-4fd2-9f71-7eaaf3f88329.png>\n",
        "\n",
        "### Data examples\n",
        "<img src = https://user-images.githubusercontent.com/43310063/188208483-f6dc9c50-3399-4607-af6b-cc3a444895e8.png>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9p21KQBwdo_g"
      },
      "source": [
        "# Custom dataset in PyTorch\n",
        "### If you make *custom dataset*, you can use dataloader in pytorch\n",
        "<br>\n",
        "\n",
        "PyTorch `DATASETS` & `DATALOADERS` : <br>\n",
        "\n",
        "- `torch.utils.data.DataLoader` and `torch.utils.data.Dataset` that allow you to use pre-loaded datasets as well as your own data. <br>\n",
        "- `Dataset` stores the samples and their corresponding labels.\n",
        "- `DataLoader` wraps an iterable around the Dataset to enable easy access to the samples.\n",
        "<br><br>\n",
        "\n",
        "- Creating a Custom Dataset for your files<br>\n",
        "    - A custom Dataset class must implement three functions: `__init__`, `__len__,` and `__getitem__`.\n",
        "    - The `__init__` function is run once when instantiating the Dataset object. We initialize the directory containing the images, the annotations file, and both transforms (covered in more detail in the next section).\n",
        "    - The `__len__` function returns the number of samples in our dataset.\n",
        "    - The `__getitem__` function loads and returns a sample from the dataset at the given index `idx`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UWgFc7HlwvTq"
      },
      "outputs": [],
      "source": [
        "\n",
        "class BostonDataset(Dataset):  # from torch.utils.data import Dataset\n",
        "    def __init__(self, features, targets, train_mean=None, train_std=None):\n",
        "        self.features = torch.Tensor(features)\n",
        "        self.targets = torch.Tensor(targets).reshape(-1, 1)\n",
        "\n",
        "        # Standard Scaler using train_data's mean and train_data's std\n",
        "        if (train_mean is not None) and (train_std is not None):\n",
        "            self.features = (self.features - train_mean) / train_std\n",
        "\n",
        "    def __len__(self):  # return length of dataset\n",
        "        return len(self.targets)\n",
        "\n",
        "    def __getitem__(self, idx):  # return data with index(idx)\n",
        "        X = self.features[idx, :] \n",
        "        y = self.targets[idx]\n",
        "        \n",
        "        return X, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyyO6erTDtD6"
      },
      "source": [
        "# $K$-fold cross validation\n",
        "\n",
        "- Lecture note has details (Week2-MLP)<br>\n",
        "\n",
        "- $K$-fold cross validation procedure\n",
        "    1. Divide the training dataset into k-parts\n",
        "    2. Use k-1 parts as training set and 1 part as validation set\n",
        "    3. Repeat the procedure K times, rotating the validation set\n",
        "    4. Average validation errors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzW6JscQaZpJ"
      },
      "source": [
        "# (Assignment) $K$-fold cross validation\n",
        "\n",
        "- Write your code in loops \n",
        "- Do not use any external library\n",
        "\n",
        "<br>\n",
        "\n",
        "return `rets` that is list <br> <br>\n",
        "`rets` form : \\\\\n",
        "ã€€ã€€ã€€ã€€[[train_index_list_1, validation_index_list_1], \\\\\n",
        "ã€€ã€€ã€€ã€€[train_index_list_2, validation_index_list_2], \\\\\n",
        "ã€€ã€€ã€€ã€€... \\\\\n",
        "ã€€ã€€ã€€ã€€[train_index_list_k, validation_index_list_k]] \\\\\n",
        "<br>\n",
        "len(`rets`) : k \\\\\n",
        "`rets`[0] : [train_index_list, validation_index_list]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZjeuxS0_JWYz"
      },
      "outputs": [],
      "source": [
        "def k_fold_data(dataset, k):\n",
        "    rets = [] # list will have k-fold data, example rets[[train_index_list_1, validation_index_list_1], ... [train_index_list_k, validation_index_list_k]]\n",
        "    fold_size = len(dataset) // k\n",
        "    for i in range(k):\n",
        "        #### TODO : WRITE YOUR CODE IN THIS LOOP & COMPLETE k-fold ####\n",
        "        validation_idx = [idx for idx in range(i*fold_size,i*fold_size+fold_size)]\n",
        "        train_idx = [idx for idx in range(len(dataset)) if idx not in validation_idx]\n",
        "        rets.append([train_idx, validation_idx])\n",
        "\n",
        "        # form of return variable :\n",
        "        # len(rets) : k \n",
        "        # rets[0] : [train_index_list, validation_index_list]\n",
        "        # example : \n",
        "        #   rets : [[[fold_size, ... len of dataset], [0, 1, .. fold_size-1],\n",
        "        #           [train_index_list_2, validation_index_list_2],\n",
        "        #           ...\n",
        "        #           [train_index_list_k, validation_index_list_k]]\n",
        "        #######################################################\n",
        "    return rets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZw4iuUiaGWv"
      },
      "source": [
        "# Load dataset & Split dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEelKv0wRwD1",
        "outputId": "519c9512-afde-4b44-ff50-65ad1ee32d31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(404, 13) (102, 13) (404,) (102,)\n"
          ]
        }
      ],
      "source": [
        "# Load Boston house prices dataset\n",
        "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
        "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
        "\n",
        "X_data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
        "y_data = raw_df.values[1::2, 2]\n",
        "\n",
        "X_data = X_data.astype(np.float32)\n",
        "y_data = y_data.astype(np.float32)\n",
        "\n",
        "# Split the dataset into a training dataset and a test dataset\n",
        "test_size = 0.2\n",
        "train_len = int(X_data.shape[0] * (1-test_size))\n",
        "\n",
        "X_train, X_test = X_data[: train_len, :], X_data[train_len:, :]\n",
        "y_train, y_test = y_data[: train_len], y_data[train_len:]\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFXDJCke1OOh"
      },
      "source": [
        "# Custom activation function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IfbCvDU1SVa"
      },
      "source": [
        "## Activation function without trainable parameter\n",
        "- sigmoid activation function\n",
        "### $\\text{Sigmoid}(x) = \\sigma(x) = \\frac{1}{1+\\text{exp}(-x)}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lTSS2Sdr1iF3"
      },
      "outputs": [],
      "source": [
        "def sigmoid_(x):\n",
        "    return 1 / (1 + torch.exp(-x))\n",
        "\n",
        "class sigmoid(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return sigmoid_(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvNdaCfW1SPp"
      },
      "source": [
        "## Activation function with implementing forward and backward step\n",
        "- tanh activation function\n",
        "### $\\text{tanh} = \\frac{\\text{exp}(x) - \\text{exp}(-x)}{\\text{exp}(x) + \\text{exp}(-x)}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4gyujK6r1it7"
      },
      "outputs": [],
      "source": [
        "def tanh_(x):\n",
        "    return (torch.exp(x) - torch.exp(-x)) / (torch.exp(x) + torch.exp(-x))\n",
        "\n",
        "class tanh(Function):  # from torch.autograd import Function\n",
        "    \n",
        "    @staticmethod  # python decorator\n",
        "    def forward(ctx, x):\n",
        "        ctx.save_for_backward(x)\n",
        "        \n",
        "        output = tanh_(x)\n",
        "        \n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        # At the top of backward unpack saved_tensors and initialize all gradients w.r.t. inputs to None.\n",
        "\n",
        "        x, = ctx.saved_tensors\n",
        "\n",
        "        if ctx.needs_input_grad[0]:\n",
        "            grad_input = 1 - tanh_(x) ** 2 # derivative of tanh(x) : 1 - tanh^2(x)\n",
        "\n",
        "        return grad_input * grad_output\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPsOJexs1jRk"
      },
      "source": [
        "# (Assignment) Activation function with implementing forward and backward step\n",
        "\n",
        "- Write activation function, forward and backward step\n",
        "- Swish (paper : https://arxiv.org/abs/1710.05941)\n",
        "### $\\text{swish}(x) = x * \\text{Sigmoid}(\\beta x)$\n",
        "- $\\beta$ is a constant or trainable parameter\n",
        "- In this practice, $\\beta$ is fixed to 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RjSJ5bOx1nKQ"
      },
      "outputs": [],
      "source": [
        "class swish(Function):\n",
        "    \n",
        "    @staticmethod  # python decorator\n",
        "    def forward(ctx, x):\n",
        "        #### TODO : WRITE YOUR CODE ####\n",
        "        ctx.save_for_backward(x)\n",
        "\n",
        "        output = x * sigmoid_(x)\n",
        "        \n",
        "        return output\n",
        "        ################################\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        # At the top of backward unpack saved_tensors and initialize all gradients w.r.t. inputs to None.\n",
        "        #### TODO : WRITE YOUR CODE ####\n",
        "        x, = ctx.saved_tensors\n",
        "\n",
        "        if ctx.needs_input_grad[0]:\n",
        "            grad_input = x * sigmoid_(x) + sigmoid_(x) * (1 - x * sigmoid_(x))\n",
        "        \n",
        "        return  grad_input * grad_output # initialize all gradients w.r.t. inputs to None.\n",
        "        ################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuDwc0ZOdxJt"
      },
      "source": [
        "# Neural Network model class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "n-onkZDOLdCQ"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, n_feature, n_hidden, n_output, dropout_rate=0.2):\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(n_feature, n_hidden)\n",
        "        self.a1 = swish.apply  # activation function we implemented\n",
        "        \n",
        "        self.fc2 = nn.Linear(n_hidden, n_hidden)\n",
        "        self.a2 = swish.apply\n",
        "\n",
        "        self.fc3 = nn.Linear(n_hidden, n_output)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_rate) # dropout with dropout_rate\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.a1(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.a2(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuDS5iU4fby2"
      },
      "source": [
        "# Training \n",
        "- Training with $K$-fold\n",
        "- Hyperparameter optimization (HPO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5u0OCtce1s3V",
        "outputId": "60f5f321-82c6-461d-f42a-452803f6b287"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of hyperparameter candidates : 27\n",
            "[{'dropout_rate': 0.1, 'learning_rate': 0.001, 'weight_decay': 0.7}, {'dropout_rate': 0.1, 'learning_rate': 0.001, 'weight_decay': 0.8}, {'dropout_rate': 0.1, 'learning_rate': 0.001, 'weight_decay': 0.9}, {'dropout_rate': 0.1, 'learning_rate': 0.005, 'weight_decay': 0.7}, {'dropout_rate': 0.1, 'learning_rate': 0.005, 'weight_decay': 0.8}, {'dropout_rate': 0.1, 'learning_rate': 0.005, 'weight_decay': 0.9}, {'dropout_rate': 0.1, 'learning_rate': 0.01, 'weight_decay': 0.7}, {'dropout_rate': 0.1, 'learning_rate': 0.01, 'weight_decay': 0.8}, {'dropout_rate': 0.1, 'learning_rate': 0.01, 'weight_decay': 0.9}, {'dropout_rate': 0.2, 'learning_rate': 0.001, 'weight_decay': 0.7}, {'dropout_rate': 0.2, 'learning_rate': 0.001, 'weight_decay': 0.8}, {'dropout_rate': 0.2, 'learning_rate': 0.001, 'weight_decay': 0.9}, {'dropout_rate': 0.2, 'learning_rate': 0.005, 'weight_decay': 0.7}, {'dropout_rate': 0.2, 'learning_rate': 0.005, 'weight_decay': 0.8}, {'dropout_rate': 0.2, 'learning_rate': 0.005, 'weight_decay': 0.9}, {'dropout_rate': 0.2, 'learning_rate': 0.01, 'weight_decay': 0.7}, {'dropout_rate': 0.2, 'learning_rate': 0.01, 'weight_decay': 0.8}, {'dropout_rate': 0.2, 'learning_rate': 0.01, 'weight_decay': 0.9}, {'dropout_rate': 0.3, 'learning_rate': 0.001, 'weight_decay': 0.7}, {'dropout_rate': 0.3, 'learning_rate': 0.001, 'weight_decay': 0.8}, {'dropout_rate': 0.3, 'learning_rate': 0.001, 'weight_decay': 0.9}, {'dropout_rate': 0.3, 'learning_rate': 0.005, 'weight_decay': 0.7}, {'dropout_rate': 0.3, 'learning_rate': 0.005, 'weight_decay': 0.8}, {'dropout_rate': 0.3, 'learning_rate': 0.005, 'weight_decay': 0.9}, {'dropout_rate': 0.3, 'learning_rate': 0.01, 'weight_decay': 0.7}, {'dropout_rate': 0.3, 'learning_rate': 0.01, 'weight_decay': 0.8}, {'dropout_rate': 0.3, 'learning_rate': 0.01, 'weight_decay': 0.9}]\n"
          ]
        }
      ],
      "source": [
        "from itertools import product\n",
        "\n",
        "# training hyperparameters used to HPO\n",
        "hpo_vals = {  # our search space for HPO\n",
        "    'learning_rate': [0.001, 0.005, 0.01],\n",
        "    'weight_decay' : [0.7, 0.8, 0.9],\n",
        "    'dropout_rate' : [0.1, 0.2, 0.3],\n",
        "}\n",
        "hp_candidates = []\n",
        "\n",
        "items = sorted(hpo_vals.items())\n",
        "keys, vals = zip(*items)\n",
        "\n",
        "# product do cartesian product, so it creates all combinations\n",
        "for v in product(*vals):  \n",
        "    hp_candidates.append(dict(zip(keys, v)))\n",
        "\n",
        "print(\"Length of hyperparameter candidates :\", len(hp_candidates))\n",
        "print(hp_candidates)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2NpiPTvK11KX"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# model hyperparameters\n",
        "n_features = X_train.shape[1] # the number of features in input\n",
        "n_hidden = 256\n",
        "n_output = 1\n",
        "\n",
        "# training hyperparameters\n",
        "batch_size = 32\n",
        "num_epochs = 20\n",
        "k_folds = 5\n",
        "\n",
        "hpo_results = []\n",
        "\n",
        "# for standard scaler\n",
        "train_mean = X_train.mean(axis=0)  # mean for each features\n",
        "train_std = X_train.std(axis=0)  # stdandard deviation for each features\n",
        "\n",
        "train_dataset = BostonDataset(X_train, y_train, train_mean, train_std)\n",
        "test_dataset = BostonDataset(X_test, y_test, train_mean, train_std)\n",
        "\n",
        "# Loops for each hyper-parameter combination\n",
        "# Train model using k-fold with a hyper-parameter combination\n",
        "for hp in hp_candidates:\n",
        "\n",
        "    # Load hyper-parameters for training\n",
        "    learning_rate = hp[\"learning_rate\"]\n",
        "    weight_decay = hp[\"weight_decay\"]\n",
        "    dropout_rate = hp[\"dropout_rate\"]\n",
        "\n",
        "    validation_logs = [[] for _ in range(k_folds)]\n",
        "\n",
        "    # Loops for each fold\n",
        "    for fold_idx, data_idx in enumerate(k_fold_data(train_dataset, k_folds)):\n",
        "        model = MLP(n_features, n_hidden, n_output, dropout_rate).to(device) # new MLP model for each fold\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay) # use weight decay to prevent overfitting\n",
        "        criterion = nn.MSELoss() # use Mean Squared Error, because it is a regression problem that predicts house prices\n",
        "\n",
        "        train_idx, validation_idx = data_idx  # results of k_fold_data function\n",
        "\n",
        "        # divide train_dataset into train_subtset and validation_subset using the indexes that are results of k_fold_data function\n",
        "        train_subset = Subset(train_dataset, train_idx)\n",
        "        validation_subset = Subset(train_dataset, validation_idx)\n",
        "\n",
        "        train_dataloader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
        "        validation_dataloader = DataLoader(validation_subset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        validation_losses = []\n",
        "\n",
        "        model.train()\n",
        "        for epoch in range(num_epochs):\n",
        "            train_loss = 0.\n",
        "            for i, (inputs, targets) in enumerate(train_dataloader):\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "                optimizer.zero_grad() \n",
        "                # sets the gradients of all optimized torch.Tensors to zero before starting to do backpropagation\n",
        "                # By default, PyTorch accumulates the gradients. Accumulating process is convinient while training RNN\n",
        "\n",
        "                y_pred = model(inputs)\n",
        "\n",
        "                loss = criterion(y_pred, targets)\n",
        "                loss.backward()  # Computes the gradient of current tensor\n",
        "                optimizer.step()  # Performs a single optimization step (parameter update)\n",
        "\n",
        "                train_loss += loss.item() * inputs.shape[0]\n",
        "\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                validation_loss = 0.\n",
        "                for i, (inputs, targets) in enumerate(validation_dataloader):\n",
        "                    inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "                    y_pred = model(inputs)\n",
        "\n",
        "                    validation_loss += criterion(y_pred, targets).item() * inputs.shape[0]\n",
        "\n",
        "                validation_logs[fold_idx].append(validation_loss / len(validation_subset))\n",
        "\n",
        "    # Validation score is calculated by averaging the results of each folds\n",
        "    validation_score = np.mean(validation_logs)\n",
        "\n",
        "    hpo_results.append([*hp.values(), validation_score])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEOruxMVggkN"
      },
      "source": [
        "### Hyperparameter tuning results\n",
        "\n",
        "- Low validation score is better in our practice, because we use the Mean Squared Error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 896
        },
        "id": "3UIEHVOWFY_t",
        "outputId": "e4e4525b-4800-4bf2-d95e-7a06e06d3131"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    dropout_rate  learning_rate  weight_decay  validation_score\n",
              "7            0.1          0.010           0.8         39.800587\n",
              "26           0.3          0.010           0.9         39.854391\n",
              "8            0.1          0.010           0.9         40.387150\n",
              "6            0.1          0.010           0.7         41.171849\n",
              "25           0.3          0.010           0.8         41.762989\n",
              "16           0.2          0.010           0.8         42.443032\n",
              "17           0.2          0.010           0.9         42.509139\n",
              "24           0.3          0.010           0.7         42.741940\n",
              "15           0.2          0.010           0.7         44.459867\n",
              "4            0.1          0.005           0.8         48.851973\n",
              "5            0.1          0.005           0.9         49.549105\n",
              "13           0.2          0.005           0.8         49.990368\n",
              "22           0.3          0.005           0.8         50.154889\n",
              "21           0.3          0.005           0.7         50.622771\n",
              "3            0.1          0.005           0.7         50.684772\n",
              "14           0.2          0.005           0.9         50.936382\n",
              "12           0.2          0.005           0.7         51.270770\n",
              "23           0.3          0.005           0.9         52.994360\n",
              "1            0.1          0.001           0.8        140.395864\n",
              "18           0.3          0.001           0.7        144.485567\n",
              "19           0.3          0.001           0.8        146.508845\n",
              "11           0.2          0.001           0.9        147.079405\n",
              "9            0.2          0.001           0.7        148.914937\n",
              "10           0.2          0.001           0.8        149.845349\n",
              "0            0.1          0.001           0.7        150.070967\n",
              "2            0.1          0.001           0.9        153.099897\n",
              "20           0.3          0.001           0.9        153.904738"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-79ecbab8-bfe1-46a8-b827-168a03c60afb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dropout_rate</th>\n",
              "      <th>learning_rate</th>\n",
              "      <th>weight_decay</th>\n",
              "      <th>validation_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.1</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.8</td>\n",
              "      <td>39.800587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.3</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.9</td>\n",
              "      <td>39.854391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.1</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.9</td>\n",
              "      <td>40.387150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.1</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.7</td>\n",
              "      <td>41.171849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.3</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.8</td>\n",
              "      <td>41.762989</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.2</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.8</td>\n",
              "      <td>42.443032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.2</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.9</td>\n",
              "      <td>42.509139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.3</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.7</td>\n",
              "      <td>42.741940</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.2</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.7</td>\n",
              "      <td>44.459867</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.1</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.8</td>\n",
              "      <td>48.851973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.1</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.9</td>\n",
              "      <td>49.549105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.2</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.8</td>\n",
              "      <td>49.990368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.3</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.8</td>\n",
              "      <td>50.154889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.3</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.7</td>\n",
              "      <td>50.622771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.1</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.7</td>\n",
              "      <td>50.684772</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.2</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.9</td>\n",
              "      <td>50.936382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.2</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.7</td>\n",
              "      <td>51.270770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.3</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.9</td>\n",
              "      <td>52.994360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.8</td>\n",
              "      <td>140.395864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.3</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.7</td>\n",
              "      <td>144.485567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.3</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.8</td>\n",
              "      <td>146.508845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.9</td>\n",
              "      <td>147.079405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.7</td>\n",
              "      <td>148.914937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.2</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.8</td>\n",
              "      <td>149.845349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.7</td>\n",
              "      <td>150.070967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.9</td>\n",
              "      <td>153.099897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.3</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.9</td>\n",
              "      <td>153.904738</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-79ecbab8-bfe1-46a8-b827-168a03c60afb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-79ecbab8-bfe1-46a8-b827-168a03c60afb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-79ecbab8-bfe1-46a8-b827-168a03c60afb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "cols = [*keys, \"validation_score\"]\n",
        "hpo_df = pd.DataFrame(hpo_results, columns=cols)\n",
        "hpo_sorted_df = hpo_df.sort_values(by=\"validation_score\")  # sort validation socre in ascending order because we used mean squared error\n",
        "hpo_sorted_df\n",
        "\n",
        "# Lower validation score is better\n",
        "# In our search space, the top row of dataframe is the best hyperparameter combination"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b--FifHX2CLh"
      },
      "source": [
        "# Train and Test model with best hyperparameter setting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GoO0rvY2Gqv"
      },
      "source": [
        "## Train model with best hyperparameter using full training dataset without $K$-fold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Em7sd0NGg7-k"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "fTwA5nl3fQ9r"
      },
      "outputs": [],
      "source": [
        "# function for plotting loss\n",
        "def plot_loss(loss_list):\n",
        "    clear_output(True) # clear output in executing cell\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    plt.ylabel(\"Train Loss\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.plot(loss_list)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "ZuTeMpkq2K-L",
        "outputId": "01e22331-315e-46ab-cf4e-42924dd52d2e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# training hyperparameters\n",
        "batch_size = 32\n",
        "num_epochs = 100\n",
        "\n",
        "# Use best hyperparameter setting, The top row of dataframe is the best hyperparameter\n",
        "learning_rate = hpo_sorted_df.iloc[0].learning_rate\n",
        "weight_decay = hpo_sorted_df.iloc[0].weight_decay\n",
        "dropout_rate = hpo_sorted_df.iloc[0].dropout_rate\n",
        "\n",
        "model_with_hpo = MLP(n_features, n_hidden, n_output, dropout_rate).to(device)\n",
        "optimizer = torch.optim.Adam(model_with_hpo.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "criterion = nn.MSELoss() # using Mean Squared Error\n",
        "\n",
        "# use full training dataset without K-fold\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "train_logs = []\n",
        "\n",
        "model_with_hpo.train()\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0.\n",
        "    for i, (inputs, targets) in enumerate(train_dataloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        y_pred = model_with_hpo(inputs)\n",
        "\n",
        "        loss = criterion(y_pred, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * inputs.shape[0]\n",
        "\n",
        "    train_logs.append(train_loss / len(train_dataset))\n",
        "    \n",
        "    if epoch % 10 == 0:\n",
        "        plot_loss(train_logs)\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3MpdrsAXtLl",
        "outputId": "350f0adb-425e-40d0-dbe5-682288da40a9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20.804073522586634, 13.596839725381077)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "train_logs[-1], np.array(train_logs).min()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1iSnBou2Gnf"
      },
      "source": [
        "## Test model with best hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icmjnbgF2Lfm",
        "outputId": "b2235a10-6eda-4e5d-dd45-598ce008a460"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average test log mse = 16.960024814979704\n"
          ]
        }
      ],
      "source": [
        "test_logs = []\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "criterion = nn.MSELoss(reduction='sum') # sum loss using Mean Squared Error\n",
        "\n",
        "model_with_hpo.eval()\n",
        "with torch.no_grad():\n",
        "    for i, (inputs, targets) in enumerate(test_dataloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        y_pred = model_with_hpo(inputs)\n",
        "        test_logs.append(criterion(y_pred, targets).item())\n",
        "\n",
        "print(f'average test log mse = {sum(test_logs)/ len(test_dataset)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GL-DlCXu6gnC"
      },
      "source": [
        "# References\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html\n",
        "- https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
        "- https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
        "- https://github.com/scikit-learn/scikit-learn/blob/36958fb24/sklearn/model_selection/_search.py#L1021\n",
        "- https://towardsdatascience.com/extending-pytorch-with-custom-activation-functions-2d8b065ef2fa\n",
        "- https://pytorch.org/docs/stable/notes/extending.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1nALDNewqHW"
      },
      "source": [
        "# Test part\n",
        "***Do not change*** this part and ***Do not add*** cell below this part<br>\n",
        "- This part is not for students\n",
        "- Do not use & add below cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOc667_fvKt2",
        "outputId": "d23e096d-9d3f-4ed5-a7e5-3ae73079339a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/autograd/gradcheck.py:653: UserWarning: Input #0 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. \n",
            "  f'Input #{idx} requires gradient and '\n"
          ]
        }
      ],
      "source": [
        "from torch.autograd import gradcheck\n",
        "\n",
        "input = (torch.rand(32, 13, dtype=torch.float, requires_grad=True)).to(device)\n",
        "\n",
        "test = gradcheck(model_with_hpo, input, eps=1e-3, atol=1e-2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVphOEKQwb1U",
        "outputId": "62cae7b3-e38a-4e5d-c47a-5de417e6f08a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "print(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tbP_QJEw32O",
        "outputId": "744f1238-a1e5-4072-db2f-1540f2a0d4f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16.960024814979704\n"
          ]
        }
      ],
      "source": [
        "print(sum(test_logs)/ len(test_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBFs_cHUa-_Y",
        "outputId": "1c1a8d8b-351c-4587-b6bc-1b6d0adbcbf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "k_test = 5\n",
        "k_fold_test = k_fold_data(input, k_test)\n",
        "\n",
        "check = True\n",
        "\n",
        "if len(k_fold_test) != k_test:    \n",
        "    check = False\n",
        "else:\n",
        "    for fold in k_fold_test:\n",
        "        if len(fold[1]) != (input.shape[0] // k_test):\n",
        "            check = False\n",
        "            break\n",
        "\n",
        "print(check)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.10 ('pdl')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "d19aa5a20e88fe80248d4b0c2d7d27a7c9d54896ad8b5c06765854ce70c72cb9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}